{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NTU_ML_2019_hw7.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMoWgqqYbFwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Install Kaggle library\n",
        "!pip install -q kaggle\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEpMA6jthEDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod 770 /content/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgdf7_K2hKdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /root/.kaggle/\n",
        "!cp /content/kaggle.json /root/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlCvblmzhKfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsK4NhhdhKjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download -c ml2019spring-hw7"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyqkcjJonN3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip images.zip\n",
        "!unzip test_case.csv.zip\n",
        "!unzip visualization.npy.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sG1mfyZNcFQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -zxvf ./Aberdeen.tar.tgz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ05Qy00iGYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np \n",
        "from skimage.io import imread, imsave \n",
        "from skimage import transform\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WWI2PIUh--s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(PAYH) ->  list :\n",
        "    filelist =[f for f in os.listdir(PAYH) if not f.startswith('.') ]  \n",
        "    # Record the shape of images\n",
        "    img_shape = imread(os.path.join(PAYH,filelist[0])).shape \n",
        "\n",
        "    img_data = []\n",
        "    for filename in filelist:\n",
        "        tmp = imread(os.path.join(PATH,filename))  \n",
        "        img_data.append(transform.resize(tmp, (128,128,3) ) .flatten())\n",
        "    return img_data \n",
        "def process(M): \n",
        "    M -= np.min(M)\n",
        "    M /= np.max(M)\n",
        "    M = (M * 255).astype(np.uint8)\n",
        "    return M\n",
        "\n",
        "# Calculate mean & Normalize\n",
        "class PCA(object):\n",
        "    def __init__(self,  compnent):\n",
        "        self.compnent = compnent\n",
        "    def fit(self , train_data):\n",
        "        self.mean = np.mean(train_data, axis = 0)  \n",
        "        train_data -= self.mean \n",
        "        # Use SVD to find the eigenvectors not use np.linalg.eig , cause not square matriz \n",
        "        self.u, self.s, v = np.linalg.svd(training_data.T, full_matrices = False)  \n",
        "        self.u=self.u[:,:self.compnent]\n",
        "        self.s=self.s[:self.compnent]\n",
        "        return \n",
        "    def infer(self,test_data ):\n",
        "        test = test_data- self.mean\n",
        "        return np.dot( np.dot(test , self.u) ,self.u.T) \n",
        "\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPdgSUQ9h-5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "PATH = './Aberdeen'   \n",
        "img_data = load_data(PATH)\n",
        "training_data = np.array(img_data).astype('float32')\n",
        "pca = PCA(compnent = 5 )\n",
        "pca.fit(training_data)\n",
        "test_image = ['1.jpg','10.jpg','22.jpg','37.jpg','72.jpg'] \n",
        "test_image_path =[os.path.join(PATH,t) for t in test_image ]\n",
        "test_data =list()\n",
        "test_data.extend([transform.resize(imread(file), (128,128,3) ).flatten()  for file in test_image_path ])\n",
        "test_data = np.array(test_data).astype('float32')\n",
        "test_result = pca.infer(test_data) \n",
        "###  show()\n",
        "plt.imshow( test_data[0].reshape(128,128,3)) \n",
        "plt.show()\n",
        "plt.imshow(process (test_result)[0].reshape(128,128,3))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbkR1aWG7oft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu9hc3Wo7oUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# standard library\n",
        "import argparse\n",
        "import csv\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "# other library\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "# PyTorch library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils import data \n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkSHMnlL7oPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess\n",
        "class Dataset(data.Dataset):\n",
        "    def __init__(self, imgae_dir):\n",
        "        self.total_img = []\n",
        "        for i in range(1, 40001):\n",
        "            print(\"loading image %d/40000\" % i, end='\\r')\n",
        "            fname = os.path.join(imgae_dir, \"%06d.jpg\" % (i))\n",
        "            img = Image.open(fname)\n",
        "            img.load()\n",
        "            row = np.asarray(img)\n",
        "            self.total_img.append(row)\n",
        "            \n",
        "        # since at pytorch conv layer, input=(N, C, H, W)\n",
        "        self.total_img = np.transpose(np.array(self.total_img, dtype=float), (0, 3, 1, 2))\n",
        "        # normalize\n",
        "        self.total_img = (self.total_img ) / 255.0\n",
        "        #np.random.shuffle(self.total_img)\n",
        "        self.train_img = self.total_img[:35501]\n",
        "        self.val_img = self.total_img[35501:]\n",
        "        print(\"=== total image shape:\",  self.total_img.shape)\n",
        "        # shape = (40000, 3, 32, 32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.total_img)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return(self.total_img[index])\n",
        "def process(M): \n",
        "    M -= np.min(M)\n",
        "    M /= np.max(M)\n",
        "    M = (M * 255).astype(np.uint8)\n",
        "    return M\n",
        "    \n",
        "imgae_dir ='./images/'\n",
        "dataset = Dataset(imgae_dir)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FxMpatZ7n2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Construction\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "def guassian_noise(input ,device ,mean = 0 ,stddev = 0.01 ):\n",
        "    return torch.clamp (input + Variable(torch.randn(input.size()).to(device) * stddev),0,1   )\n",
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, image_shape, latent_dim):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.shape = image_shape\n",
        "        self.latent_dim = latent_dim\n",
        "        # CNN  output_shape = (image_shape-kernel_size+2*padding)/stride + 1\n",
        "        self.encoder = nn.Sequential(  # batch ,3 , N ,N\n",
        "            nn.Conv2d(3, 16, 3, padding=1 ,stride = 1 ),  #batch , 16 ,  32, 32\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.MaxPool2d(2, 2),   # batch , 16 , 16,16\n",
        "\n",
        "            nn.Conv2d(16, 64, 3, padding=1,stride =1 ),  # batch , 64 , 16,16\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.MaxPool2d(2, 2),   # batch , 16 ,8,8\n",
        "\n",
        "            nn.Conv2d(64, 256, 3, padding=1,stride =1 ),  # batch , 64 , 8,8\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(2, 2),    # batch , 256 , 4,4\n",
        "        )\n",
        "        # assume output shape is (Batch, channel, 1, 1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(256*4*4 , self.latent_dim)\n",
        "        self.fc2 = nn.Linear(self.latent_dim, 256*4*4)\n",
        "        #self.fc3 = nn.Linear(self.h_dim , self.latent_dim)\n",
        "        # ConvTranspose2d output_shape = (input_shape-1)*stride + output_padding -2*padding +kernel_size\n",
        "        self.decoder = nn.Sequential(\n",
        "           # TODO: define your own structure\n",
        "           nn.ConvTranspose2d(256*4*4, 256 ,3 , stride=1 ),   # batch , 64 ,3,3\n",
        "           nn.ReLU(),\n",
        "           nn.BatchNorm2d(256),\n",
        "           nn.Upsample(scale_factor=2, mode='bilinear',align_corners=False) , #batch,64,6,6\n",
        "           \n",
        "           nn.ConvTranspose2d(256 ,64 ,2 ,stride=1 ) ,    # batch , 64, 7,7\n",
        "           nn.ReLU(),\n",
        "           nn.BatchNorm2d(64),\n",
        "           nn.Upsample(scale_factor=2, mode='bilinear' ,align_corners=False) ,#batch,64,14,14\n",
        "           \n",
        "\n",
        "           nn.ConvTranspose2d(64 ,16 ,2 ,stride=1 ) , #batch ,16 , 15,15\n",
        "           nn.ReLU(), \n",
        "           nn.BatchNorm2d(16),\n",
        "           nn.Upsample(scale_factor=2, mode='bilinear' ,align_corners=False) , #batch,64,30,30\n",
        "           \n",
        "           nn.ConvTranspose2d(16 ,3 ,3 ,stride=1 ) , #batch ,16 , 32,32\n",
        "           nn.ReLU(), \n",
        "        )\n",
        "   # def reparameterize(self, mu, logvar):\n",
        "    #    sigma = torch.exp(logvar)\n",
        "     #   std_z = torch.from_numpy(np.random.normal(0, 1, size=sigma.size())).float()\n",
        "      #  return mu + sigma * Variable(std_z, requires_grad=False).to(device) \n",
        "     \n",
        "    #def encode(self,x):\n",
        "      #  h = F.relu(self.fc1(x))\n",
        "      #  return self.fc2(h) , self.fc3(h)\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # flatten\n",
        "        x = x.view(len(x), -1) # batch , (channel*pixel*pixel)\n",
        "        \n",
        "        #mu , log_var = self.encode(x)\n",
        "        #encoded = self.reparameterize( mu ,log_var )\n",
        "\n",
        "        encoded = self.fc1(x)\n",
        "        x = F.relu(self.fc2(encoded))  \n",
        "\n",
        "        #encoded = self.fc2(x)\n",
        "        #x = F.relu(self.fc2(encoded))  \n",
        "        x = x.view(-1, 256*4*4, 1, 1) \n",
        "        x = self.decoder(x)\n",
        "        return encoded, x #,mu, log_var\n",
        "\n",
        "train = data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "validation = data.DataLoader(dataset.val_img, batch_size=500, shuffle=False)\n",
        "total_data = data.DataLoader(dataset, batch_size=256, shuffle=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZEzd5Zk8EDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWKJy8sFd5CO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "model = AutoEncoder(image_shape=dataset.__getitem__(0).shape , latent_dim = 128 )\n",
        "model.to(device)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, \n",
        "                             weight_decay=1e-5)\n",
        "                             \n",
        "num_epochs=60\n",
        "model_path = '/content/drive/My'' ''Drive/Colab_model/'\n",
        "if os.path.isfile(model_path+'autoencoder_model'):\n",
        "    #model.load_state_dict(torch.load(model_path+'autoencoder_model')) #for gpu \n",
        "    model.load_state_dict(torch.load(model_path+'autoencoder_model' , map_location=torch.device('cpu')))\n",
        "\n",
        "best_loss =100\n",
        "val_loss=[]\n",
        "train_loss =[]\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start_time = time.time()\n",
        "    total_loss  = 0\n",
        "    model.train()\n",
        "    for idx,image in enumerate(train):\n",
        "        image = image.to(device , dtype=torch.float)\n",
        "        image_noise = guassian_noise(image , device)\n",
        "        _, reconsturct = model(image_noise)\n",
        "\n",
        "        #kl_div = 0.5 * torch.mean( 1- mu.pow(2) - log_var.exp() + log_var )\n",
        "        loss = criterion(reconsturct, image) #- kl_div\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += (loss.item() / len(train))\n",
        "        print(loss.item(),end='\\r')\n",
        "    print ('[%03d/%03d] %2.2f sec(s) ' % (epoch+1, num_epochs, \\\n",
        "                (time.time() - epoch_start_time) ))\n",
        "    print(\"\\n Training | Loss :%.4f \" % total_loss)\n",
        "    train_loss.append(total_loss)\n",
        "    # validation set\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        " \n",
        "    with torch.no_grad():\n",
        "        for idx, image in enumerate(validation):\n",
        "                image = image.to(device , dtype=torch.float)\n",
        "                image_noise = guassian_noise(image , device)\n",
        "                _, reconstruct = model(image_noise)\n",
        "               # kl_div = 0.5 * torch.mean(1- mu.pow(2) - log_var.exp() + log_var )\n",
        "                loss = criterion(reconstruct, image)# - kl_div\n",
        "                total_loss += (loss.item() / len(validation))\n",
        "        print(\"[%03d/%03d]%2.2f sec Validation | Loss:%.4f \" \\\n",
        "                % (epoch+1, num_epochs,(time.time() - epoch_start_time), total_loss) )\n",
        "        val_loss.append(total_loss)\n",
        "        if(best_loss > total_loss):\n",
        "            best_loss = total_loss\n",
        "            print(\"saving model with val loss %.4f...\\n\" % total_loss)\n",
        "            torch.save(model.state_dict(),model_path+'autoencoder_model')\n",
        "            print('Model Saved!!! ')\n",
        "    if(epoch%5==0):\n",
        "        \n",
        "        index = 20\n",
        "        sh =  np.transpose(dataset.__getitem__(index), (  1,2, 0) )\n",
        "        plt.imshow(sh)\n",
        "        plt.show()\n",
        "        model.eval()\n",
        "        _ , reconsturct = model(\\\n",
        "        torch.FloatTensor( np.expand_dims( dataset.__getitem__(index),axis=0 ) ).cuda()  )\n",
        "\n",
        "\n",
        "        sh =  np.transpose(process ( reconsturct[0].cpu().data.numpy()), (  1,2, 0) )\n",
        "        plt.imshow(sh)\n",
        "        plt.show()\n",
        "        \n",
        "plt.plot(range(num_epochs),train_loss)\n",
        "plt.plot(range(num_epochs),val_loss)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrS62XxMd48V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') \n",
        "model = AutoEncoder(image_shape=dataset.__getitem__(0).shape , latent_dim = 128 )\n",
        "model.to(device)\n",
        "model_path = '/content/drive/My'' ''Drive/Colab_model/'\n",
        "if os.path.isfile(model_path+'autoencoder_model'):\n",
        "    model.load_state_dict(torch.load(model_path+'autoencoder_model'))\n",
        "\n",
        "def clustering(model, device, loader,n_iter, reduced_dim):\n",
        "    model.eval()\n",
        "    da = list()\n",
        "    #latent_vec = torch.tensor([]).to(device, dtype=torch.float)\n",
        "    for idx, image in enumerate(loader):\n",
        "        print(\"predict %d / %d\" % (idx, len(loader)) , end='\\r')\n",
        "        image = image.to(device, dtype=torch.float)\n",
        "        latent, _ = model(image)\n",
        "        #latent_vec = torch.cat((latent_vec, latent), dim=0)\n",
        "        latent = latent.cpu().detach().numpy()\n",
        "        da.extend(latent)\n",
        "    #print(latent_vec.shape)\n",
        "    # shape = (40000, latent_dim)\n",
        "    # n_components < 4\n",
        "    #tsne = TSNE(n_components=reduced_dim, verbose=1, perplexity=50, n_iter=n_iter)\n",
        "    #latent_vec = tsne.fit_transform(np.array(da))\n",
        "\n",
        "    pca = PCA(n_components=reduced_dim, copy=False, whiten=True, svd_solver='full')\n",
        "    latent_vec = pca.fit_transform(np.array(da))\n",
        "\n",
        "    kmeans = KMeans(n_clusters=2, random_state=0, max_iter=n_iter).fit(latent_vec)\n",
        "    return kmeans.labels_\n",
        "def read_test_case(path):\n",
        "    dm = pd.read_csv(path)\n",
        "    img1 = dm['image1_name']\n",
        "    img2 = dm['image2_name']\n",
        "    test_case = np.transpose(np.array([img1, img2]))\n",
        "    return test_case\n",
        "def prediction(label, test_case, output):\n",
        "    result = []\n",
        "    for i in range(len(test_case)):\n",
        "        index1, index2 = int(test_case[i][0])-1, int(test_case[i][1])-1\n",
        "        if label[index1] != label[index2]:\n",
        "            result.append(0)\n",
        "        else:\n",
        "            result.append(1)\n",
        "    \n",
        "    result = np.array(result)\n",
        "    with open(output, 'w') as f:\n",
        "        f.write(\"id,label\\n\")\n",
        "        for i in range(len(test_case)):\n",
        "            f.write(\"%d,%d\\n\" % (i, result[i]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leNd3CanA9Po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label = clustering( model ,device , total_data,n_iter=300 ,reduced_dim=128 )\n",
        "test_case = read_test_case('./test_case.csv')\n",
        "prediction(label , test_case , './submit.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHjnIacj7nw_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = np.load('./visualization.npy')/255.0\n",
        "test_set = np.transpose(np.array(test_set, dtype=float), (0, 3, 1, 2))\n",
        "test = data.DataLoader(test_set, batch_size=10, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "model.to(device , dtype=torch.float)\n",
        "criterion = nn.MSELoss()\n",
        "test_loss=0\n",
        "for t in (test):\n",
        "    t = t.to(device , dtype=torch.float)\n",
        "    _ , img = model(t)\n",
        "    loss = criterion(img,t)\n",
        "    test_loss +=loss\n",
        "print(test_loss/len(test))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz4WUOAAlcg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}